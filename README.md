# README

The replication package of paper *Are your comments outdated? Towards automatically detecting code-comment consistency*.

## Download resources

- Download data sets and other related resources from [here](https://drive.google.com/drive/folders/1VT6rXgyVDECpbBuwI1gNGu9VtDsidCkH?usp=share_link)
  - changes_drop_duplicates.zip, this is the changed java source code resource we extracted.
  - features_max_paper.zip, these are the features we used in the paper.
  - expand_features_sum.zip, these are the characteristics of our continued work and expansion after submission, using sum function respectively to calculate the similarity between token and sentence.
  - expand_features_max.zip, these are the characteristics of our continued work and expansion after submission, using max function respectively to calculate the similarity between token and sentence.
  - corpus.txt, this is the resource generated by using the method mentioned in the paper to train code and comment word vectors.
  - vocab.npy, the token dictionary we extracted.
  - word_vector.model, the word vector we trained through 'corpus.txt'.

## Installation

```
#enviroment requirements.yaml
```

```bash
conda env create -f requirements.yaml
```

## View the extracted features

- features_max_paper.csv, these are the sample features we used in the paper. Here we calculate the similarity between token and sentence through max function.
- The following two feature csv files are extracted after we greatly expanded the data set after the paper submission. Because the data set is greatly expanded and the positive and negative samples are unbalanced, the evaluation results are reduced. Use sum function and max function respectively to calculate the similarity between token and sentence.
  - expand_features_sum.csv
  - expand_features_max.csv

## Infer and Eval

- Open outdate_ Predict folder, run the main program to detect outdated comments on all samples
- Open other folders beginning with outDatePredict, and run the main program with folders ending with corresponding classifiers to test the performance of different classifiers
- Open the outdate_predict_only_comment_change folder and run the main.py program to test RQ3 (samples with only commented changes)
- Note that we do not use the de duplication function in these steps, because we have de duplicated the duplicate samples during data processing, and then extract features from the de duplicated samples. If the extracted features are duplicate, we consider them as valid samples

## utils

Tools for data cleaning, original commit processing, feature extraction, word vector training, etc. (including CCSet extraction, change extraction, CCSet with only annotation change extraction, feature extraction, token feature extraction, sample de-duplication, and local csv generation)

**NOTE**: In our paper, each model was trained and evaluated 10 times, and the reported results are the best performance of the experiments.
So the outputs of the above commands would be different from those reported in our paper. 

**！！！** We collect data from different projects. Theoretically, we do not need to de duplicate after feature extraction. In the paper, we ignore the problem of de duplication, but after submission, we find that the extracted features have duplicate examples (even if we also introduce the similarity of word vector changes). The possible reason is that the similarity of code and its changes in the same project is relatively high, so the extracted features will also repeat, We ignored this in the paper. We retrained the model for the features after de duplication. The model is about 5 points lower than the data in the paper. We open source the features after de duplication and the features in the paper on github.
